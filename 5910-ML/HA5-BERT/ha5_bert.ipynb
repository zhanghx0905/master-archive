{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Npiyxtr9f0U",
        "outputId": "a5a2ea40-ba2e-4a68-a6cd-fb09f3a70a5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\文档\\PG-CSIT\\ML\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from transformers import AdamW\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "byD173eF9f0Y"
      },
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert, drop_rate=0.5):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        #pass the inputs to the model  \n",
        "        x = self.bert(sent_id, attention_mask=mask)[1]\n",
        "        x = self.fc1(x)\n",
        "        #x dim 512\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv_dTar_9f0Z",
        "outputId": "3f3b28ec-0964-4be1-8750-b27cee112c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences = 50000\n",
            "positive    0.5\n",
            "negative    0.5\n",
            "Name: sentiment, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "print(\"Number of sentences =\", df.shape[0])\n",
        "# check class distribution\n",
        "print(df['sentiment'].value_counts(normalize = True))\n",
        "\n",
        "labels = np.zeros_like(df['sentiment'])\n",
        "labels[df['sentiment']==\"positive\"] = 1\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['review'], labels, \n",
        "                                    random_state=2018, \n",
        "                                    test_size=5500, \n",
        "                                    stratify=labels)\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                  random_state=2018, \n",
        "                                  test_size=500, \n",
        "                                  stratify=temp_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T0bh37Sm-SJN"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 256\n",
        "lr = 1e-3\n",
        "drop_rate=0.7\n",
        "batch_size = 16\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUnwR6NJ9f0a",
        "outputId": "c8d812c3-2318-4e46-c095-710b8d3c3ba9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "d:\\文档\\PG-CSIT\\ML\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y77uMuCC9f0b",
        "outputId": "2308e884-7134-40ee-de2c-d1800cf20f74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\文档\\PG-CSIT\\ML\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = lr)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QOKCE-8p9f0c"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "    model.train()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # clear previously calculated gradients \n",
        "        model.zero_grad()        \n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward() #GRADIENT\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "        # append the model predictions\n",
        "        total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "    print(\"\\nEvaluating...\")\n",
        "\n",
        "    # deactivate dropout layers\n",
        "    model.eval() #DROP OUT\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "\n",
        "          # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "          # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "          # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCP6JY8A9f0d",
        "outputId": "3d2e3a0b-bfc0-4d98-b6e5-7144fc0fc185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    50  of  2,782.\n",
            "  Batch   100  of  2,782.\n",
            "  Batch   150  of  2,782.\n",
            "  Batch   200  of  2,782.\n",
            "  Batch   250  of  2,782.\n",
            "  Batch   300  of  2,782.\n",
            "  Batch   350  of  2,782.\n",
            "  Batch   400  of  2,782.\n",
            "  Batch   450  of  2,782.\n",
            "  Batch   500  of  2,782.\n",
            "  Batch   550  of  2,782.\n",
            "  Batch   600  of  2,782.\n",
            "  Batch   650  of  2,782.\n",
            "  Batch   700  of  2,782.\n",
            "  Batch   750  of  2,782.\n",
            "  Batch   800  of  2,782.\n",
            "  Batch   850  of  2,782.\n",
            "  Batch   900  of  2,782.\n",
            "  Batch   950  of  2,782.\n",
            "  Batch 1,000  of  2,782.\n",
            "  Batch 1,050  of  2,782.\n",
            "  Batch 1,100  of  2,782.\n",
            "  Batch 1,150  of  2,782.\n",
            "  Batch 1,200  of  2,782.\n",
            "  Batch 1,250  of  2,782.\n",
            "  Batch 1,300  of  2,782.\n",
            "  Batch 1,350  of  2,782.\n",
            "  Batch 1,400  of  2,782.\n",
            "  Batch 1,450  of  2,782.\n",
            "  Batch 1,500  of  2,782.\n",
            "  Batch 1,550  of  2,782.\n",
            "  Batch 1,600  of  2,782.\n",
            "  Batch 1,650  of  2,782.\n",
            "  Batch 1,700  of  2,782.\n",
            "  Batch 1,750  of  2,782.\n",
            "  Batch 1,800  of  2,782.\n",
            "  Batch 1,850  of  2,782.\n",
            "  Batch 1,900  of  2,782.\n",
            "  Batch 1,950  of  2,782.\n",
            "  Batch 2,000  of  2,782.\n",
            "  Batch 2,050  of  2,782.\n",
            "  Batch 2,100  of  2,782.\n",
            "  Batch 2,150  of  2,782.\n",
            "  Batch 2,200  of  2,782.\n",
            "  Batch 2,250  of  2,782.\n",
            "  Batch 2,300  of  2,782.\n",
            "  Batch 2,350  of  2,782.\n",
            "  Batch 2,400  of  2,782.\n",
            "  Batch 2,450  of  2,782.\n",
            "  Batch 2,500  of  2,782.\n",
            "  Batch 2,550  of  2,782.\n",
            "  Batch 2,600  of  2,782.\n",
            "  Batch 2,650  of  2,782.\n",
            "  Batch 2,700  of  2,782.\n",
            "  Batch 2,750  of  2,782.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "SAVING MODEL\n",
            "\n",
            "Training Loss: 0.602\n",
            "Validation Loss: 0.528\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    50  of  2,782.\n",
            "  Batch   100  of  2,782.\n",
            "  Batch   150  of  2,782.\n",
            "  Batch   200  of  2,782.\n",
            "  Batch   250  of  2,782.\n",
            "  Batch   300  of  2,782.\n",
            "  Batch   350  of  2,782.\n",
            "  Batch   400  of  2,782.\n",
            "  Batch   450  of  2,782.\n",
            "  Batch   500  of  2,782.\n",
            "  Batch   550  of  2,782.\n",
            "  Batch   600  of  2,782.\n",
            "  Batch   650  of  2,782.\n",
            "  Batch   700  of  2,782.\n",
            "  Batch   750  of  2,782.\n",
            "  Batch   800  of  2,782.\n",
            "  Batch   850  of  2,782.\n",
            "  Batch   900  of  2,782.\n",
            "  Batch   950  of  2,782.\n",
            "  Batch 1,000  of  2,782.\n",
            "  Batch 1,050  of  2,782.\n",
            "  Batch 1,100  of  2,782.\n",
            "  Batch 1,150  of  2,782.\n",
            "  Batch 1,200  of  2,782.\n",
            "  Batch 1,250  of  2,782.\n",
            "  Batch 1,300  of  2,782.\n",
            "  Batch 1,350  of  2,782.\n",
            "  Batch 1,400  of  2,782.\n",
            "  Batch 1,450  of  2,782.\n",
            "  Batch 1,500  of  2,782.\n",
            "  Batch 1,550  of  2,782.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\文档\\PG-CSIT\\ML\\HA\\HA5-BERT\\ha5_bert.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000007?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000007?line=12'>13</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000007?line=13'>14</a>\u001b[0m train_loss, _ \u001b[39m=\u001b[39m train()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000007?line=15'>16</a>\u001b[0m \u001b[39m#evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000007?line=16'>17</a>\u001b[0m valid_loss, _ \u001b[39m=\u001b[39m evaluate()\n",
            "\u001b[1;32md:\\文档\\PG-CSIT\\ML\\HA\\HA5-BERT\\ha5_bert.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000006?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy(preds, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000006?line=31'>32</a>\u001b[0m \u001b[39m# add on to the total loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000006?line=32'>33</a>\u001b[0m total_loss \u001b[39m=\u001b[39m total_loss \u001b[39m+\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000006?line=34'>35</a>\u001b[0m \u001b[39m# backward pass to calculate the gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%96%87%E6%A1%A3/PG-CSIT/ML/HA/HA5-BERT/ha5_bert.ipynb#ch0000006?line=35'>36</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m#GRADIENT\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        print(\"SAVING MODEL\")\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "interpreter": {
      "hash": "c1bc5876cc3251270ea58a3c31369234ddbc8f02a3bcbb60a81cacbc1dc34c5a"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
